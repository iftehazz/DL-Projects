{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6690251,"sourceType":"datasetVersion","datasetId":3857852},{"sourceId":6699621,"sourceType":"datasetVersion","datasetId":3861661},{"sourceId":7081071,"sourceType":"datasetVersion","datasetId":3259629},{"sourceId":7084394,"sourceType":"datasetVersion","datasetId":4081608},{"sourceId":7095473,"sourceType":"datasetVersion","datasetId":4089274},{"sourceId":7095504,"sourceType":"datasetVersion","datasetId":4089301},{"sourceId":7099646,"sourceType":"datasetVersion","datasetId":4092406},{"sourceId":7099655,"sourceType":"datasetVersion","datasetId":4092415},{"sourceId":7099665,"sourceType":"datasetVersion","datasetId":4092424}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\n\ntorch.__version__","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:01.628699Z","iopub.status.busy":"2023-12-02T14:58:01.628271Z","iopub.status.idle":"2023-12-02T14:58:01.635843Z","shell.execute_reply":"2023-12-02T14:58:01.634654Z","shell.execute_reply.started":"2023-12-02T14:58:01.628662Z"}},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":["'2.0.0'"]},"metadata":{}}]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:01.647704Z","iopub.status.busy":"2023-12-02T14:58:01.647400Z","iopub.status.idle":"2023-12-02T14:58:01.659561Z","shell.execute_reply":"2023-12-02T14:58:01.658633Z","shell.execute_reply.started":"2023-12-02T14:58:01.647679Z"}},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":["'cuda'"]},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport zipfile\n\nfrom pathlib import Path\nimport random\nimport shutil\nfrom shutil import copyfile\n\nimport requests\nimport pandas as pd\n\n# Setup path to data folder\ndata_path = Path(\"/ham10000-splitted/Images splitted/Splitted\")\nimage_path = data_path # FOLDER path = splitted dataset into Train & Test\n\n# Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:01.828817Z","iopub.status.busy":"2023-12-02T14:58:01.828485Z","iopub.status.idle":"2023-12-02T14:58:01.840596Z","shell.execute_reply":"2023-12-02T14:58:01.839766Z","shell.execute_reply.started":"2023-12-02T14:58:01.828786Z"}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torchvision.transforms.functional import InterpolationMode\n\n\nIMG_SIZE = 600\n\n# Create transform pipeline manually   \ndata_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomPerspective(distortion_scale=0.2),\n    transforms.Normalize([0.5018, 0.5015, 0.5013], [0.1029, 0.0985, 0.0807]),\n])     \n\n\n\n\n\n\ndata_transform_test = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5018, 0.5015, 0.5013], [0.1029, 0.0985, 0.0807]),\n])  \n\n\n# Use ImageFolder to create dataset(s)\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\n\ntest_data = datasets.ImageFolder(root=test_dir,\n                                 transform=data_transform_test,\n                                )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torchvision.transforms.functional import InterpolationMode\n\nIMG_SIZE = 224\n\n\ndata_transform2 = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n#     transforms.RandomRotation((-120,120)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n\n])\n\n# print(f\"Manually created transforms: {data_transform}\")\n\n\n\n\n\ndata_transform_test2 = transforms.Compose([\n#     transforms.CenterCrop(200),\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\n\n\ntrain_data2 = datasets.ImageFolder(root=train_dir,\n                                  transform=data_transform2,\n                                  target_transform=None)\n\n\ntest_data2 = datasets.ImageFolder(root=test_dir,\n                                 transform=data_transform_test2,\n                                )\n\n# print(f\"Train data:\\n{train_data2}\\nTest data:\\n{test_data2}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 224\n\n# Create transform pipeline manually   \ndata_transform3 = transforms.Compose([\n#     transforms.CenterCrop(200),\n#     transforms.RandAugment(num_ops = 8, \n#                            magnitude = 9, \n#                            num_magnitude_bins = 31, \n#                            interpolation = InterpolationMode.BILINEAR, \n#                            ),\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n#     transforms.RandomRotation((-120,120)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n#     transforms.RandomAffine(degrees=360, scale=(1.0, 1.3)),\n#     transforms.RandomAdjustSharpness(sharpness_factor=0),\n#     transforms.RandomAdjustSharpness(sharpness_factor=2),\n    transforms.RandomPerspective(distortion_scale=0.2),\n    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    \n    # Calculated for train data\n    transforms.Normalize([0.5018, 0.5015, 0.5013], [0.1029, 0.0985, 0.0807]),\n])     \n\n\n\n\n\n\n\ndata_transform_test3 = transforms.Compose([\n#     transforms.CenterCrop(200),\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n\n    # Calculated for test data\n    transforms.Normalize([0.5018, 0.5015, 0.5013], [0.1029, 0.0985, 0.0807]),\n])  \n\n\n# Use ImageFolder to create dataset(s)\ntrain_data3 = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform3, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\n\ntest_data3 = datasets.ImageFolder(root=test_dir,\n                                 transform=data_transform_test3,\n                                )\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Auto Transform**","metadata":{}},{"cell_type":"code","source":"# Get class names as a list\nclass_names = train_data.classes\nclass_names","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:03.886595Z","iopub.status.busy":"2023-12-02T14:58:03.886225Z","iopub.status.idle":"2023-12-02T14:58:03.893672Z","shell.execute_reply":"2023-12-02T14:58:03.892737Z","shell.execute_reply.started":"2023-12-02T14:58:03.886568Z"}},"execution_count":113,"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":["['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']"]},"metadata":{}}]},{"cell_type":"code","source":"# Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:03.895165Z","iopub.status.busy":"2023-12-02T14:58:03.894773Z","iopub.status.idle":"2023-12-02T14:58:03.904705Z","shell.execute_reply":"2023-12-02T14:58:03.903837Z","shell.execute_reply.started":"2023-12-02T14:58:03.895132Z"}},"execution_count":114,"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":["{'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}"]},"metadata":{}}]},{"cell_type":"code","source":"# Check the lengths\nlen(train_data), len(test_data)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:03.914972Z","iopub.status.busy":"2023-12-02T14:58:03.914713Z","iopub.status.idle":"2023-12-02T14:58:03.929695Z","shell.execute_reply":"2023-12-02T14:58:03.928831Z","shell.execute_reply.started":"2023-12-02T14:58:03.914949Z"}},"execution_count":116,"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":["(8017, 1998)"]},"metadata":{}}]},{"cell_type":"code","source":"# Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\n\nBATCH_SIZE = 32\n\n\n\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=BATCH_SIZE, # how many samples per batch?\n                              num_workers=4, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True,\n                              pin_memory=True,\n\n                              ) # shuffle the data?\n\n\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=BATCH_SIZE, \n                             num_workers=4, \n                             shuffle=False,\n                             pin_memory=True,\n                             ) # don't usually need to shuffle testing data\n\n\nlen(train_dataloader), len(test_dataloader)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:03.937351Z","iopub.status.busy":"2023-12-02T14:58:03.937020Z","iopub.status.idle":"2023-12-02T14:58:03.954320Z","shell.execute_reply":"2023-12-02T14:58:03.953429Z","shell.execute_reply.started":"2023-12-02T14:58:03.937327Z"}},"execution_count":118,"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":["(251, 63)"]},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, WeightedRandomSampler\n\nBATCH_SIZE = 32\n\n\n\ntrain_dataloader2 = DataLoader(dataset=train_data2,\n                              batch_size=BATCH_SIZE,\n                              num_workers=4,\n                              shuffle=True,\n                              pin_memory=True,\n                              )\n\n\n\ntest_dataloader2 = DataLoader(dataset=test_data2,\n                             batch_size=BATCH_SIZE,\n                             num_workers=4,\n                             shuffle=False,\n                             pin_memory=True,\n                             )\n\n\nlen(train_dataloader), len(test_dataloader)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:03.955895Z","iopub.status.busy":"2023-12-02T14:58:03.955586Z","iopub.status.idle":"2023-12-02T14:58:03.967912Z","shell.execute_reply":"2023-12-02T14:58:03.967041Z","shell.execute_reply.started":"2023-12-02T14:58:03.955858Z"}},"execution_count":119,"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":["(251, 63)"]},"metadata":{}}]},{"cell_type":"code","source":"train_dataloader3 = DataLoader(dataset=train_data3,\n                              batch_size=BATCH_SIZE,\n                              num_workers=4,\n                              shuffle=True,\n                              pin_memory=True,\n                              )\n\n\n\ntest_dataloader3 = DataLoader(dataset=test_data3,\n                             batch_size=BATCH_SIZE,\n                             num_workers=4,\n                             shuffle=False,\n                             pin_memory=True,\n                             )\n\n\nlen(train_dataloader3), len(test_dataloader3)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:03.969862Z","iopub.status.busy":"2023-12-02T14:58:03.969053Z","iopub.status.idle":"2023-12-02T14:58:03.983841Z","shell.execute_reply":"2023-12-02T14:58:03.983024Z","shell.execute_reply.started":"2023-12-02T14:58:03.969828Z"}},"execution_count":120,"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":["(251, 63)"]},"metadata":{}}]},{"cell_type":"markdown","source":"# **Ensemble Models : FB, ViT, maxViT, EffNetB5, Inception, DenseNet**","metadata":{}},{"cell_type":"code","source":"model_fb = torch.hub.load('facebookresearch/swav:main', 'resnet50').to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:04.069611Z","iopub.status.busy":"2023-12-02T14:58:04.069227Z","iopub.status.idle":"2023-12-02T14:58:04.871119Z","shell.execute_reply":"2023-12-02T14:58:04.870092Z","shell.execute_reply.started":"2023-12-02T14:58:04.069572Z"}},"execution_count":129,"outputs":[{"name":"stderr","output_type":"stream","text":"Using cache found in /root/.cache/torch/hub/facebookresearch_swav_main\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n\n  warnings.warn(\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n\n  warnings.warn(msg)\n"}]},{"cell_type":"code","source":"# Set the manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:04.872683Z","iopub.status.busy":"2023-12-02T14:58:04.872406Z","iopub.status.idle":"2023-12-02T14:58:04.877364Z","shell.execute_reply":"2023-12-02T14:58:04.876463Z","shell.execute_reply.started":"2023-12-02T14:58:04.872660Z"}},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"model_fb.fc = torch.nn.Linear(in_features=2048, \n                           out_features=7, # same number of output units as our number of classes\n                           bias=True).to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:05.136522Z","iopub.status.busy":"2023-12-02T14:58:05.136214Z","iopub.status.idle":"2023-12-02T14:58:05.458127Z","shell.execute_reply":"2023-12-02T14:58:05.457136Z","shell.execute_reply.started":"2023-12-02T14:58:05.136497Z"}},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"model_fb = nn.DataParallel(model_fb)  ### for two GPU faster computations ","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:05.459661Z","iopub.status.busy":"2023-12-02T14:58:05.459377Z","iopub.status.idle":"2023-12-02T14:58:05.464275Z","shell.execute_reply":"2023-12-02T14:58:05.463382Z","shell.execute_reply.started":"2023-12-02T14:58:05.459638Z"}},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"model_fb.load_state_dict(torch.load('/pretrained-fb-model/0.8863863863863863 acc.pth'))","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:05.465638Z","iopub.status.busy":"2023-12-02T14:58:05.465367Z","iopub.status.idle":"2023-12-02T14:58:05.594431Z","shell.execute_reply":"2023-12-02T14:58:05.593551Z","shell.execute_reply.started":"2023-12-02T14:58:05.465615Z"}},"execution_count":135,"outputs":[{"execution_count":135,"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{}}]},{"cell_type":"code","source":"import torchvision\nmodel_vit = torchvision.models.vit_b_32(pretrained=True).to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:05.596030Z","iopub.status.busy":"2023-12-02T14:58:05.595729Z","iopub.status.idle":"2023-12-02T14:58:07.063449Z","shell.execute_reply":"2023-12-02T14:58:07.062617Z","shell.execute_reply.started":"2023-12-02T14:58:05.596004Z"}},"execution_count":136,"outputs":[{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_32_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_32_Weights.DEFAULT` to get the most up-to-date weights.\n\n  warnings.warn(msg)\n"}]},{"cell_type":"code","source":"model_vit.heads = torch.nn.Sequential(\n    torch.nn.Linear(in_features=768, \n                    out_features=len(class_names), # same number of output units as our number of classes\n                    bias=True)).to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:07.065117Z","iopub.status.busy":"2023-12-02T14:58:07.064700Z","iopub.status.idle":"2023-12-02T14:58:07.071632Z","shell.execute_reply":"2023-12-02T14:58:07.070590Z","shell.execute_reply.started":"2023-12-02T14:58:07.065058Z"}},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"model_vit.load_state_dict(torch.load('/best-vit/best_vit_model.pth'))","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:07.317188Z","iopub.status.busy":"2023-12-02T14:58:07.316760Z","iopub.status.idle":"2023-12-02T14:58:07.894036Z","shell.execute_reply":"2023-12-02T14:58:07.893091Z","shell.execute_reply.started":"2023-12-02T14:58:07.317040Z"}},"execution_count":139,"outputs":[{"execution_count":139,"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{}}]},{"cell_type":"code","source":"import torchvision\nmodel_maxvit = torchvision.models.maxvit_t(pretrained=True).to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:07.902089Z","iopub.status.busy":"2023-12-02T14:58:07.901767Z","iopub.status.idle":"2023-12-02T14:58:08.812557Z","shell.execute_reply":"2023-12-02T14:58:08.811456Z","shell.execute_reply.started":"2023-12-02T14:58:07.902051Z"}},"execution_count":140,"outputs":[{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaxVit_T_Weights.IMAGENET1K_V1`. You can also use `weights=MaxVit_T_Weights.DEFAULT` to get the most up-to-date weights.\n\n  warnings.warn(msg)\n"}]},{"cell_type":"code","source":"model_maxvit.classifier._modules['5'] = nn.Linear(512, len(class_names))","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:08.820018Z","iopub.status.busy":"2023-12-02T14:58:08.819747Z","iopub.status.idle":"2023-12-02T14:58:08.828069Z","shell.execute_reply":"2023-12-02T14:58:08.827157Z","shell.execute_reply.started":"2023-12-02T14:58:08.819994Z"}},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"model_maxvit.load_state_dict(torch.load('/best-maxvit/best_maxvit_model.pth'))","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:09.345654Z","iopub.status.busy":"2023-12-02T14:58:09.345190Z","iopub.status.idle":"2023-12-02T14:58:10.026379Z","shell.execute_reply":"2023-12-02T14:58:10.025459Z","shell.execute_reply.started":"2023-12-02T14:58:09.345619Z"}},"execution_count":144,"outputs":[{"execution_count":144,"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{}}]},{"cell_type":"markdown","source":"# **EffNetB5**","metadata":{}},{"cell_type":"code","source":"import torchvision\n\nweights = torchvision.models.EfficientNet_B5_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nmodel_effnetb5 = torchvision.models.efficientnet_b5(weights=weights).to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:10.028346Z","iopub.status.busy":"2023-12-02T14:58:10.027956Z","iopub.status.idle":"2023-12-02T14:58:10.704299Z","shell.execute_reply":"2023-12-02T14:58:10.703432Z","shell.execute_reply.started":"2023-12-02T14:58:10.028313Z"}},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"# Set the manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Get the length of class_names (one output unit for each class)\noutput_shape = len(class_names)\n\n# Recreate the classifier layer and seed it to the target device\nmodel_effnetb5.classifier = torch.nn.Sequential(\n    torch.nn.Dropout(p=0.2, inplace=True), \n    torch.nn.Linear(in_features=2048, \n                    out_features=output_shape, # same number of output units as our number of classes\n                    bias=True)).to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:10.706129Z","iopub.status.busy":"2023-12-02T14:58:10.705697Z","iopub.status.idle":"2023-12-02T14:58:10.714202Z","shell.execute_reply":"2023-12-02T14:58:10.713137Z","shell.execute_reply.started":"2023-12-02T14:58:10.706068Z"}},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"model_effnetb5 = nn.DataParallel(model_effnetb5)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:10.979997Z","iopub.status.busy":"2023-12-02T14:58:10.979667Z","iopub.status.idle":"2023-12-02T14:58:10.984644Z","shell.execute_reply":"2023-12-02T14:58:10.983739Z","shell.execute_reply.started":"2023-12-02T14:58:10.979972Z"}},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"model_effnetb5.load_state_dict(torch.load('/effnetb5-ham10000-87-74/0.8773773773773774 acc.pth'))","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:10.986238Z","iopub.status.busy":"2023-12-02T14:58:10.985879Z","iopub.status.idle":"2023-12-02T14:58:11.205332Z","shell.execute_reply":"2023-12-02T14:58:11.204367Z","shell.execute_reply.started":"2023-12-02T14:58:10.986205Z"}},"execution_count":149,"outputs":[{"execution_count":149,"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{}}]},{"cell_type":"markdown","source":"# **Inception V3**","metadata":{}},{"cell_type":"code","source":"weights = torchvision.models.Inception_V3_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nmodel_inceptionv3 = torchvision.models.inception_v3(weights=weights).to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:11.206982Z","iopub.status.busy":"2023-12-02T14:58:11.206656Z","iopub.status.idle":"2023-12-02T14:58:11.546801Z","shell.execute_reply":"2023-12-02T14:58:11.545961Z","shell.execute_reply.started":"2023-12-02T14:58:11.206943Z"}},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"# Set the manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Get the length of class_names (one output unit for each class)\noutput_shape = len(class_names)\n\n# Recreate the classifier layer and seed it to the target device\nmodel_inceptionv3.fc = torch.nn.Linear(in_features=2048, \n                    out_features=output_shape, # same number of output units as our number of classes\n                    bias=True).to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:11.548430Z","iopub.status.busy":"2023-12-02T14:58:11.548125Z","iopub.status.idle":"2023-12-02T14:58:11.555535Z","shell.execute_reply":"2023-12-02T14:58:11.554477Z","shell.execute_reply.started":"2023-12-02T14:58:11.548404Z"}},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"model_inceptionv3 = nn.DataParallel(model_inceptionv3) ","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:11.824936Z","iopub.status.busy":"2023-12-02T14:58:11.824529Z","iopub.status.idle":"2023-12-02T14:58:11.830143Z","shell.execute_reply":"2023-12-02T14:58:11.829138Z","shell.execute_reply.started":"2023-12-02T14:58:11.824899Z"}},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"model_inceptionv3.load_state_dict(torch.load('/inceptionv3-ham10000-86-39/0.8638638638638638 acc.pth'))","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:11.832298Z","iopub.status.busy":"2023-12-02T14:58:11.831433Z","iopub.status.idle":"2023-12-02T14:58:11.998558Z","shell.execute_reply":"2023-12-02T14:58:11.997580Z","shell.execute_reply.started":"2023-12-02T14:58:11.832263Z"}},"execution_count":154,"outputs":[{"execution_count":154,"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{}}]},{"cell_type":"markdown","source":"# **DenseNet**","metadata":{}},{"cell_type":"code","source":"weights = torchvision.models.DenseNet121_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nmodel_densenet121 = torchvision.models.densenet121(weights=weights).to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:11.999977Z","iopub.status.busy":"2023-12-02T14:58:11.999695Z","iopub.status.idle":"2023-12-02T14:58:12.244829Z","shell.execute_reply":"2023-12-02T14:58:12.244021Z","shell.execute_reply.started":"2023-12-02T14:58:11.999952Z"}},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"# Set the manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Get the length of class_names (one output unit for each class)\noutput_shape = len(class_names)\n\n# Recreate the classifier layer and seed it to the target device\nmodel_densenet121.classifier = torch.nn.Linear(in_features=1024, \n                    out_features=output_shape, # same number of output units as our number of classes\n                    bias=True).to(device)","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:12.246651Z","iopub.status.busy":"2023-12-02T14:58:12.246329Z","iopub.status.idle":"2023-12-02T14:58:12.253003Z","shell.execute_reply":"2023-12-02T14:58:12.251955Z","shell.execute_reply.started":"2023-12-02T14:58:12.246624Z"}},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"model_densenet121 = nn.DataParallel(model_densenet121)  ","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:12.514256Z","iopub.status.busy":"2023-12-02T14:58:12.513821Z","iopub.status.idle":"2023-12-02T14:58:12.519580Z","shell.execute_reply":"2023-12-02T14:58:12.518485Z","shell.execute_reply.started":"2023-12-02T14:58:12.514218Z"}},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"model_densenet121.load_state_dict(torch.load('/densenet121-ham10000-86-34/0.8633633633633634 acc.pth'))","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:12.521378Z","iopub.status.busy":"2023-12-02T14:58:12.521003Z","iopub.status.idle":"2023-12-02T14:58:12.676895Z","shell.execute_reply":"2023-12-02T14:58:12.675947Z","shell.execute_reply.started":"2023-12-02T14:58:12.521345Z"}},"execution_count":159,"outputs":[{"execution_count":159,"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{}}]},{"cell_type":"markdown","source":"# **Ensemble**","metadata":{}},{"cell_type":"code","source":"# from going_modular import engine\n\n# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper \noptimizer = torch.optim.AdamW(params=model_fb.parameters(), \n#                               lr=2e-3, # Base LR from Table 3 for ViT-* ImageNet-1k                                    ## learning rate decre..\n                              lr = 1e-3,\n                              betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n                              weight_decay=0.05,\n#                               weight_decay=1e-5\n                              )\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Set the seeds\n# set_seeds()\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\nbest_accuracy=0.0","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:12.679141Z","iopub.status.busy":"2023-12-02T14:58:12.678470Z","iopub.status.idle":"2023-12-02T14:58:12.687105Z","shell.execute_reply":"2023-12-02T14:58:12.686061Z","shell.execute_reply.started":"2023-12-02T14:58:12.679082Z"}},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"from typing import Dict, List, Tuple\nimport torch\nfrom tqdm.auto import tqdm\n\ndef test_step(model: torch.nn.Module, \n              model2: torch.nn.Module,\n              model3: torch.nn.Module,\n              model4: torch.nn.Module, \n              model5: torch.nn.Module,\n              model6: torch.nn.Module,\n              dataloader: torch.utils.data.DataLoader,\n              dataloader2: torch.utils.data.DataLoader,\n              dataloader3: torch.utils.data.DataLoader,\n              loss_fn: torch.nn.Module,\n              device: torch.device,\n              len_test_data) -> Tuple[float, float]:\n\n    # Put model in eval mode\n    model.eval() \n    model2.eval()\n    model3.eval()\n    model4.eval() \n    model5.eval()\n    model6.eval()\n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n    \n    # Turn on inference context manager\n    with torch.inference_mode():\n\n        iterator = iter(dataloader2)\n        iterator3 = iter(dataloader3)\n        \n        for batch, (X, y) in enumerate(dataloader):\n\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n            \n            X2, y2 = next(iterator)\n            X2, y2 = X2.to(device), y2.to(device)\n            \n            X3, y3 = next(iterator3)\n            X3, y3 = X3.to(device), y3.to(device)\n\n            \n            # 1. Forward pass\n            test_pred_logits = (model(X)) + (model3(X2)) + (model4(X3)) + (model5(X)) + (model6(X3)) # model2 and 3 trained on same dataloader\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n\n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_pred_logits = test_pred_logits\n            test_acc += (test_pred_labels == y).sum().item()\n\n\n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len_test_data\n\n    # return test_loss, test_acc\n    return test_loss, test_acc\n\n\n\nepochs = 1\nfor epoch in tqdm(range(epochs)):\n        test_loss, test_acc = test_step(model=model_fb,\n                                       model2=model_vit,\n                                       model3=model_maxvit,\n                                       model4=model_effnetb5,\n                                       model5=model_inceptionv3,\n                                       model6=model_densenet121,\n                                     dataloader=test_dataloader,\n                                     dataloader2=test_dataloader2,\n                                     dataloader3=test_dataloader3,\n                                     loss_fn=loss_fn,\n                                     device=device,\n                                     len_test_data=len(test_data))\n\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n","metadata":{"execution":{"iopub.execute_input":"2023-12-02T14:58:12.688837Z","iopub.status.busy":"2023-12-02T14:58:12.688503Z","iopub.status.idle":"2023-12-02T14:59:19.118494Z","shell.execute_reply":"2023-12-02T14:59:19.117415Z","shell.execute_reply.started":"2023-12-02T14:58:12.688811Z"}},"execution_count":161,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14043ddb83344241b34d17695a8f9156","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"Epoch: 1 | test_loss: 1.0417 | test_acc: 0.9224\n"}]}]}